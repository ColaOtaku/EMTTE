{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "##TrainDataClean##\n",
    "##################\n",
    "\n",
    "## import urllib.request \n",
    "from urllib import parse \n",
    "import json \n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "# import chart_studio.plotly as py\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import math\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "mykey=u'51cffc64fd38c249e18e2a8265ad06f9'\n",
    "FilePath='E:/WorkSpace/py/Highway_analysis/data/trans/2'\n",
    "record=pd.DataFrame([])\n",
    "for root, dirs, files in os.walk(FilePath):\n",
    "    for file in files:\n",
    "        df=pd.read_csv(os.path.join(root, file),encoding='utf-8')\n",
    "        record=record.append(df)\n",
    "        \n",
    "record=record.drop(record[record['lookup.n_en_date']>=20200701].index)\n",
    "record=record.drop(record[record['lookup.n_en_date']<20200601].index)\n",
    "record=record.drop(record[record['lookup.n_trade_speed']>50].index)\n",
    "record['lookup.d_fee_length']=record['lookup.d_fee_length'].replace(0,np.nan)\n",
    "record=record.dropna()\n",
    "dateg=record.groupby('lookup.n_en_date')\n",
    "\n",
    "for date in range(20200616,20200631):\n",
    "    filename='data/trans/bydate/'+str(date)+'.csv'\n",
    "    df=dateg.get_group(date)\n",
    "    df['lookup.n_en_date']=date\n",
    "    df.to_csv(filename,encoding='utf_8_sig')\n",
    "    \n",
    "## TrainingDataClean\n",
    "##\n",
    "FilePath='data/trans/bydate/'\n",
    "record=pd.DataFrame([])\n",
    "for root, dirs, files in os.walk(FilePath):\n",
    "    for file in tqdm(files):\n",
    "        df=pd.read_csv(os.path.join(root, file),encoding='utf-8')\n",
    "        record=record.append(df)\n",
    "\n",
    "tolldata=pd.read_excel('data/收费站info.xlsx')\n",
    "tolldata.columns=['idx','id','name','lon','lat']\n",
    "tollList=tolldata['id'].tolist()\n",
    "tollList=[str(i) for i in tollList]\n",
    "mapping={}\n",
    "for i in range(0,457):\n",
    "    mapping[str(tolldata['id'][i])]=i\n",
    "rmapping=dict(map(reversed, mapping.items())) \n",
    "\n",
    "def tollmapping(x):\n",
    "    if str(x) in tollList:\n",
    "        return mapping[str(x)]\n",
    "    else:\n",
    "        return np.nan\n",
    "        \n",
    "FilePath='data/trans/bydate/'\n",
    "savePath='data/trans/cleaned/'\n",
    "for root, dirs, files in os.walk(FilePath):   \n",
    "    for file in tqdm(files):   \n",
    "        sample=pd.read_csv(os.path.join(root, file),encoding='utf-8')\n",
    "        sample=sample.drop(sample[sample['lookup.n_en_date']>=20200701].index)\n",
    "        sample=sample.drop(sample[sample['lookup.n_en_date']<20200601].index)\n",
    "        sample=sample.drop(sample[sample['lookup.n_trade_speed']>50].index)\n",
    "        sample['lookup.d_fee_length']=sample['lookup.d_fee_length'].replace(0,np.nan)\n",
    "        sample=sample.dropna()\n",
    "        \n",
    "        TransInfo=sample.loc[:,['lookup.c_card_license','lookup.n_en_date','lookup.n_en_time','lookup.n_ex_date','lookup.n_ex_time','lookup.n_en_station_id','lookup.n_ex_lane_id']]\n",
    "        TransInfo.columns=['id','endate','entime','exdate','extime','enstation','exstation']\n",
    "        TransInfo['exstation']//=100\n",
    "\n",
    "        TransInfo['enTime']=(TransInfo['endate'].apply(lambda x : str(x))+' '+TransInfo['entime'].apply(lambda x : \"{:0>6d}\".format(x))).apply(lambda x : pd.to_datetime(x))\n",
    "        TransInfo['exTime']=(TransInfo['exdate'].apply(lambda x : str(x))+' '+TransInfo['extime'].apply(lambda x : \"{:0>6d}\".format(x))).apply(lambda x : pd.to_datetime(x))\n",
    "        TransInfo['enpoint']=TransInfo['enstation'].apply(lambda x : tollmapping(x))\n",
    "        TransInfo['expoint']=TransInfo['exstation'].apply(lambda x : tollmapping(x))\n",
    "        TransInfo['period']=TransInfo['exTime']-TransInfo['enTime']\n",
    "\n",
    "        TransInfo=TransInfo.dropna()\n",
    "\n",
    "        TransInfo=TransInfo.loc[:,['id','enTime','enpoint','exTime','expoint','period']]\n",
    "        TransInfo=TransInfo.drop(TransInfo[TransInfo['period']<datetime.timedelta(days=0)].index)\n",
    "        \n",
    "        TransInfo['period']=TransInfo['period'].apply(lambda x: x.total_seconds()/60/60)\n",
    "        TransInfo['mileage']=TransInfo.apply(lambda x: dists[int(x['enpoint']),int(x['expoint'])],axis=1)\n",
    "        TransInfo['avgspeed']=TransInfo.apply(lambda x: float(format(x['mileage']/x['period'], '.2f')),axis=1)\n",
    "        \n",
    "        TransInfo=TransInfo.drop(TransInfo[TransInfo['avgspeed']>120].index)\n",
    "        TransInfo=TransInfo.drop(TransInfo[TransInfo['avgspeed']<50].index)\n",
    "        TransInfo.to_csv(savePath+file,encoding='utf_8_sig')\n",
    "\n",
    "#################\n",
    "##TestDataClean##\n",
    "#################\n",
    "        \n",
    "G=nx.read_gpickle(\"data/Graph/HzGraph.gpickle\")\n",
    "licenselist=['京','津','冀','晋','内','辽','吉','黑','沪','苏','浙','皖',\n",
    "             '闽','赣','鲁','台','豫','鄂','湘','粤','桂','琼','港','澳',\n",
    "             '渝','川','贵','云','滇','藏','陕','甘','青','宁','新']\n",
    "pd.read_csv('data/gantry/query_hive_3269.csv')\n",
    "gpos=pd.read_excel('data/TollStation/gantry.xlsx')\n",
    "\n",
    "gantry_list=[[996,1003,1007,1011,738,750,745]]\n",
    "table=[]\n",
    "for i in gantry_list[0]:\n",
    "    table.append(gpos.iloc[i,2])\n",
    "    \n",
    "test_data=pd.DataFrame([])\n",
    "FilePath='data/gantry/'\n",
    "for root, dirs, files in os.walk(FilePath): \n",
    "    for file in tqdm(files):\n",
    "        if(file=='.DS_Store'):\n",
    "            continue\n",
    "        if(file=='testdata.csv'):\n",
    "            continue\n",
    "        else:\n",
    "            df=pd.read_csv(os.path.join(root, file),encoding='utf-8')\n",
    "            df.columns=['tradeid','gid','time','cid','idx']\n",
    "            df['select']=df['gid'].apply(lambda x:x in table)\n",
    "            df=df[df['select']==True]\n",
    "            df=df.iloc[:,1:-1]\n",
    "            df['cid']=df['cid'].apply(lambda x:x[:-2])\n",
    "            df=df.iloc[:,:-1]         \n",
    "            test_data=test_data.append(df)\n",
    "test_data=test_data.reset_index(drop=True)\n",
    "\n",
    "groups=df.groupby('cid')\n",
    "g,reclen=[],0\n",
    "for label,group in groups:\n",
    "    group.drop_duplicates(subset='time',keep='first',inplace=True)\n",
    "    group.drop_duplicates(subset='gid',keep='first',inplace=True)\n",
    "    if(len(group)>1):\n",
    "        group=group.reset_index(drop=True)\n",
    "        group['time']=group['time'].apply(lambda x : pd.to_datetime(x))\n",
    "        if(group['cid'][0][0]) in licenselist:\n",
    "            reclen+=len(group)\n",
    "            group=group.sort_values(by='time')\n",
    "            group=group.reset_index(drop=True)\n",
    "            g.append(group)\n",
    "\n",
    "records=[]\n",
    "for group in g:\n",
    "#     print(group)\n",
    "    for i in range(len(group)-1):\n",
    "        records.append({'ori':gmap[group['gid'][i]],'des':gmap[group['gid'][i+1]],'cost':(group['time'][i+1]-group['time'][i]).total_seconds(),'id':group['cid'][i],'time':group['time'][i]})\n",
    "pd.DataFrame(records).to_csv('data/gantry/testdata.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import osmnx as ox\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import itertools\n",
    "from sklearn import metrics\n",
    "roadsec=[271, 15, 37, 13, 34, 291, 289]\n",
    "gmap={996:21,1003:11,1007:22,1011:465,738:397,750:23,745:12}\n",
    "\n",
    "transfer={446:268,450:367,442:327,443:324,444:162,429:377,447:269,448:93,449:292,437:68,440:207,438:254,435:174,436:175,432:164,439:235,445:247,427:11,439:235,434:117,433:173,431:342,451:368,}\n",
    "selected_path=pd.read_csv('data/path/Hz_path.csv',index_col=0)\n",
    "\n",
    "######################\n",
    "##Model Construction##\n",
    "######################\n",
    "\n",
    "G=nx.read_gpickle(\"data/Graph/HzGraph.gpickle\")\n",
    "train_data=pd.read_csv('data/trans/byhour/12:00:00.csv')\n",
    "train_data=train_data.reset_index(drop=True)\n",
    "Hzlist=[515,516,519,520,11,12,528,529,530,531,21,22,23,30,31,32,33,34,\n",
    "        37,38,39,42,43,44,45,54,78,79,80,81,82,83,84,90,91,92,112,113,\n",
    "        114,115,116,117,118,119,132,137,145,146,147,148,149,150,151,152,\n",
    "        153,154,155,156,157,158,159,160,161,162,163,164,170,171,172,173,\n",
    "        174,175,176,177,178,179,226,232,233,234,235,236,247,250,251,252,\n",
    "        253,254,255,256,257,258,263,282,283,291,293,294,295,296,297,302,\n",
    "        324,327,330,331,332,333,334,335,340,341,342,343,344,371,372,373,\n",
    "        374,375,377,379,380,381,384,385,387,388,389,393,397,420,421,422,\n",
    "        426,427,428,430,432,433,434,435,436,438,439,445,453,454,455,456,\n",
    "        459,464,465,469,470,472,475,477,483,488,496,498,502,504,505,700,\n",
    "        701,702,703,704,705,517]\n",
    "\n",
    "train_data_Hz=[]\n",
    "for item in tqdm(train_data.iterrows()):\n",
    "    item=item[1]\n",
    "    if(int(item['enpoint']) in Hzlist and int(item['expoint']) in Hzlist):\n",
    "        train_data_Hz.append(item)\n",
    "train_data_Hz=pd.DataFrame(train_data_Hz)\n",
    "\n",
    "Edge_List=[]\n",
    "for t in G.edges():\n",
    "      Edge_List.append(t)\n",
    "Edge_dict=dict([])\n",
    "for i,item in enumerate(Edge_List):\n",
    "    Edge_dict.update({item:i})\n",
    "    \n",
    "allpath=[]\n",
    "for i in tqdm(NodeList):\n",
    "    for j in NodeList:\n",
    "        if i==j or i>457 or j >457 or (not len(G[i])) or (not len(G[j])):\n",
    "            continue\n",
    "        if i in transfer.keys():\n",
    "            i=transfer[i]\n",
    "        if j in transfer.keys():\n",
    "            j=transfer[j]\n",
    "        gen=ox.distance.k_shortest_paths(G,i,j,10, weight='length')\n",
    "        genpath, genlength = itertools.tee(gen,2)\n",
    "        allpath.append({'pair':(i,j),\n",
    "                        'path':[m for m in genpath],\n",
    "                        'length':[cal_len(G,m) for m in genlength]})\n",
    "selected_path=[]\n",
    "for i in allpath:\n",
    "    paths,pathlen=[],[]\n",
    "    paths.append(i['path'][0])\n",
    "    pathlen.append(i['length'][0])\n",
    "    for j in range(len(i['path'])-1):\n",
    "        if (i['length'][j+1]-i['length'][j])/i['length'][j]>0.4:\n",
    "            break\n",
    "        paths.append(i['path'][j])\n",
    "        pathlen.append(i['length'][j])\n",
    "    selected_path.append({'pair':str(i['pair'])[1:-1],'path':paths,'length':pathlen})\n",
    "selected_path=pd.DataFrame(selected_path)\n",
    "\n",
    "NodeList=list(G.nodes())\n",
    "for i in G.edges():\n",
    "    G[i[0]][i[1]][0]['length']=ox.distance.great_circle_vec(G.nodes[i[0]]['y'],G.nodes[i[0]]['x'],G.nodes[i[1]]['y'],G.nodes[i[1]]['x'])\n",
    "    G[i[0]][i[1]][0]['miu']=0\n",
    "    G[i[0]][i[1]][0]['sigma']=0\n",
    "    G[i[0]][i[1]][0]['init']=[]\n",
    "\n",
    "init_data=train_data_Hz\n",
    "for item in tqdm(init_data.iterrows()):\n",
    "    ori,des=int(item[1]['enpoint']),int(item[1]['expoint'])\n",
    "    if ori in transfer.keys():\n",
    "        ori=transfer[ori]\n",
    "    if des in transfer.keys():\n",
    "        des=transfer[des]\n",
    "    if ori == des:\n",
    "        continue\n",
    "        \n",
    "    od_idx=selected_path[selected_path['pair']==str(ori)+', '+str(des)].index[0]\n",
    "    shortest_path=[int(i) for i in selected_path.loc[od_idx,'path'][2:-2].split('], [')[0].split(',')]\n",
    "    pathlen=float(selected_path.loc[od_idx,'length'][1:-1].split(',')[0])\n",
    "    \n",
    "    for idx in range(len(shortest_path)-1):\n",
    "        i,j=shortest_path[idx],shortest_path[idx+1]\n",
    "        if(idx==0):\n",
    "            G[i][j][0]['init']+=[(G[i][j][0]['length']/pathlen*item[1]['period'])-G.nodes[i]['ramp']/1000/30]\n",
    "        elif(idx==len(shortest_path)-2):\n",
    "            G[i][j][0]['init']+=[(G[i][j][0]['length']/pathlen*item[1]['period'])-G.nodes[j]['ramp']/1000/30] \n",
    "        else:\n",
    "            G[i][j][0]['init']+=[(G[i][j][0]['length']/pathlen*item[1]['period'])] \n",
    "for e in G.edges():\n",
    "    ei,ej=e[0],e[1]\n",
    "    if( G[ei][ej][0]['init']!=[]):\n",
    "        G[ei][ej][0]['miu']=np.mean(np.array(G[ei][ej][0]['init']))\n",
    "        G[ei][ej][0]['sigma']=np.std(np.array(G[ei][ej][0]['init']))    \n",
    "Mu=np.zeros([len(G.edges())])\n",
    "Sigma=np.zeros([len(G.edges())])\n",
    "for i,t in enumerate(G.edges()):\n",
    "    Mu[i]=G[t[0]][t[1]][0]['miu']\n",
    "    Sigma[i]=G[t[0]][t[1]][0]['sigma']\n",
    "init_Mu=Mu\n",
    "init_Sigma=Sigma\n",
    "\n",
    "Sample_Size=30\n",
    "Edge_size=len(G.edges())\n",
    "threshold=10\n",
    "ramp_para=30*1000  # 30KM/H  1KM\n",
    "\n",
    "\n",
    "##################\n",
    "##Model Training##\n",
    "##################\n",
    "\n",
    "\n",
    "\n",
    "#########\n",
    "#Model 1#\n",
    "#########\n",
    "cantidate=[21,11,22,465,397,23,12]\n",
    "opt=[]\n",
    "for i in tqdm(train_data_Hz.iterrows()):\n",
    "    item=i[1]\n",
    "    ori,des=int(item['enpoint']),int(item['expoint'])\n",
    "    if ori in cantidate and des in cantidate:\n",
    "        opt.append(item)\n",
    "opt=pd.DataFrame(opt)\n",
    "ret=[]\n",
    "for i in roadsec:\n",
    "    orig,dest = Edge_List[i][0],Edge_List[i][1]\n",
    "    result=[]\n",
    "    for i in opt.iterrows():\n",
    "        item=i[1]\n",
    "        ori,des=int(item['enpoint']),int(item['expoint'])\n",
    "        if ori ==orig and des ==dest:\n",
    "            result.append(item)\n",
    "    ret.append(pd.DataFrame(result))\n",
    "SMu=[]\n",
    "for i in range(len(roadsec)):\n",
    "    orig,dest = Edge_List[roadsec[i]][0],Edge_List[roadsec[i]][1]\n",
    "    if(ret[i].empty):\n",
    "        SMu.append(0)\n",
    "    else:\n",
    "        SMu.append(np.mean(np.array(ret[i]['period']))-G.nodes[orig]['ramp']/1000/30-G.nodes[dest]['ramp']/1000/30)\n",
    "ResultM1=SMu\n",
    "\n",
    "#########\n",
    "#Model 2#\n",
    "#########\n",
    "ResultM2=Mu[roadsec]\n",
    "\n",
    "#########\n",
    "#Model 3#\n",
    "#########\n",
    "def cal_alpha(path,Edge_size):\n",
    "    alpha=np.zeros([Edge_size,1])\n",
    "    for i in range(len(path)-1):\n",
    "        alpha[Edge_dict[(path[i],path[i+1])]]=1\n",
    "    return alpha\n",
    "\n",
    "def GenerateCov(Mean,Sigma,Sample_Size,Edge_size):\n",
    "    Mean=Mu  #联合分布的/mu,/Mean\n",
    "    Cov=np.zeros([Edge_size,Edge_size])\n",
    "    row, col = np.diag_indices_from(Cov)\n",
    "    Cov[row,col] = Mu*Mu\n",
    "\n",
    "    return Cov\n",
    "\n",
    "update_data=init_data\n",
    "\n",
    "for it in range(5):    \n",
    "    #calculate Cov\n",
    "    Cov=GenerateCov(Mu,Sigma,Sample_Size,Edge_size)\n",
    "    #path inferer\n",
    "    update_matrix=[]\n",
    "    for record in tqdm(update_data.iterrows()):\n",
    "        record=record[1]\n",
    "        pathset=[]\n",
    "        ori,des,cost=int(record['enpoint']),int(record['expoint']),float(record['period'])\n",
    "        if ori in transfer.keys():\n",
    "            ori=transfer[ori]\n",
    "        if des in transfer.keys():\n",
    "            des=transfer[des]\n",
    "        if ori == des:\n",
    "            continue\n",
    "        pair_id=selected_path[selected_path['pair']==str(ori)+', '+str(des)].index[0]\n",
    "        for j in selected_path.iloc[pair_id,1][2:-2].split('], ['):\n",
    "            pathset.append([int(k) for k in j.split(',')])\n",
    "        if(len(pathset)>1):\n",
    "            pathset.pop(0)\n",
    "        path_MAPE=[]\n",
    "        Edge_sample=np.random.normal(loc=Mu, scale=Sigma, size=(Sample_Size,Edge_size)).T\n",
    "        offset=G.nodes[ori]['ramp']/ramp_para+G.nodes[des]['ramp']/ramp_para\n",
    "        for path in pathset:\n",
    "            alpha=cal_alpha(path,Edge_size)\n",
    "            Z=np.sum(Edge_sample*alpha,axis=0)+offset\n",
    "            path_MAPE.append(100/Sample_Size*np.sum(np.abs(Z-record['period'])/record['period']))\n",
    "        alpha=cal_alpha(pathset[np.argmin(np.array(path_MAPE))],Edge_size) #记录MAPE最小的路径alpha\n",
    "        \n",
    "    #sample under constrain\n",
    "        cost=cost-G.nodes[ori]['ramp']/ramp_para-G.nodes[des]['ramp']/ramp_para\n",
    "        y_without_constrain=np.random.multivariate_normal(Mu, Cov, 1).T\n",
    "        para=(cost-alpha.T.dot(y_without_constrain))/(alpha.T.dot(Cov)).dot(alpha)\n",
    "        X=y_without_constrain+para*Cov.dot(alpha)\n",
    "        \n",
    "        update_matrix.append((alpha*X).reshape(Edge_size))\n",
    "    #update parameter\n",
    "    update_matrix=np.array(update_matrix)\n",
    "    last_mu,last_sigma=Mu.copy(),Sigma.copy()\n",
    "    for idx,e in enumerate(update_matrix.T):\n",
    "        new_sample=e[e.nonzero()]\n",
    "        if new_sample.size>threshold:\n",
    "            Mu[idx],Sigma[idx]=np.mean(new_sample),np.std(new_sample)\n",
    "ResultM3=Mu[roadsec]\n",
    "\n",
    "#########\n",
    "#Model 4#\n",
    "#########\n",
    "def cal_weight(e):\n",
    "    e=[i/max(e) for i in e]\n",
    "    e_=[np.exp(1/i) for i in e]\n",
    "    sums=sum(e_)\n",
    "    return [i/sums for i in e_]\n",
    "batch_size=int(len(init_data)/5)\n",
    "Mu=init_Mu\n",
    "Sigma=init_Sigma\n",
    "\n",
    "for it in range(5):    \n",
    "    for i in range(5):\n",
    "        left,right=i*batch_size,(i+1)*batch_size\n",
    "        update_data=init_data.iloc[left:right,:]\n",
    "        #calculate Cov\n",
    "        Cov=GenerateCov(Mu,Sigma,Sample_Size,Edge_size)\n",
    "\n",
    "        #path inferer\n",
    "        update_matrix=[]\n",
    "        W_list=[]\n",
    "        for record in tqdm(update_data.iterrows()):\n",
    "            record=record[1]\n",
    "            pathset=[]\n",
    "            ori,des,cost=int(record['enpoint']),int(record['expoint']),float(record['period'])\n",
    "            if ori in transfer.keys():\n",
    "                ori=transfer[ori]\n",
    "            if des in transfer.keys():\n",
    "                des=transfer[des]\n",
    "            if ori == des:\n",
    "                continue\n",
    "            pair_id=selected_path[selected_path['pair']==str(ori)+', '+str(des)].index[0]\n",
    "            for j in selected_path.iloc[pair_id,1][2:-2].split('], ['):\n",
    "                pathset.append([int(k) for k in j.split(',')])\n",
    "            if(len(pathset)>1):\n",
    "                pathset.pop(0)\n",
    "            path_MAPE=[]\n",
    "\n",
    "            Edge_sample=np.random.normal(loc=Mu, scale=Sigma, size=(Sample_Size,Edge_size)).T\n",
    "            offset=G.nodes[ori]['ramp']/ramp_para+G.nodes[des]['ramp']/ramp_para\n",
    "            for path in pathset:\n",
    "                alpha=cal_alpha(path,Edge_size)\n",
    "                Z=np.sum(Edge_sample*alpha,axis=0)+offset\n",
    "                path_MAPE.append(100/Sample_Size*np.sum(np.abs(Z-record['period'])/record['period']))\n",
    "            W=cal_weight(path_MAPE)\n",
    "        #sample under constrain\n",
    "            for idx,path in enumerate(pathset):\n",
    "                            \n",
    "                alpha=cal_alpha(path,Edge_size) \n",
    "                cost=cost-G.nodes[ori]['ramp']/ramp_para-G.nodes[des]['ramp']/ramp_para\n",
    "                y_without_constrain=np.random.multivariate_normal(Mu, Cov, 1).T\n",
    "                para=(cost-alpha.T.dot(y_without_constrain))/(alpha.T.dot(Cov)).dot(alpha)\n",
    "                X=y_without_constrain+para*Cov.dot(alpha)\n",
    "\n",
    "                update_matrix.append((alpha*X).reshape(Edge_size))\n",
    "            W_list=W_list+W\n",
    "        #update parameter\n",
    "        update_matrix=np.array(update_matrix)\n",
    "        W_list=np.array(W_list)\n",
    "        last_mu,last_sigma=Mu.copy(),Sigma.copy()\n",
    "        \n",
    "        for idx,e in enumerate(update_matrix.T):\n",
    "            sample=e[e.nonzero()]\n",
    "            weight=W_list[e.nonzero()]\n",
    "            if sample.size>threshold:\n",
    "                Mu[idx]=np.sum(sample*weight)/np.sum(weight)\n",
    "                Sigma[idx]=sample.size/(sample.size-1)*np.sum(weight*(sample-Mu[idx])**2)/np.sum(weight)\n",
    "ResultM4=Mu[roadsec]\n",
    "\n",
    "############\n",
    "#Model Test#\n",
    "############\n",
    "test_data=pd.read_csv('data/gantry/testdata.csv',index_col=0)\n",
    "Groups=test_data.groupby(['ori','des'])\n",
    "testSet=dict([])\n",
    "for idx,g in Groups:\n",
    "    testSet[(gmap[idx[0]],gmap[idx[1]])]=list(g['cost'])\n",
    "data_=[np.array(testSet[i]) for i in list(testSet.keys())]\n",
    "tdata=[np.mean(i) for i in data_]\n",
    "tdata=np.array(tdata)\n",
    "tdata=tdata*scaler\n",
    "\n",
    "def cal_MAE(x,y):\n",
    "    return np.mean(np.abs(x-y))\n",
    "def cal_MRE(x,y):\n",
    "    return np.mean(np.abs(x-y)/x)\n",
    "\n",
    "#########\n",
    "#Model 1#\n",
    "#########\n",
    "print(cal_MAE(ResultM1[1:5]*3600,tdata[1:5]))\n",
    "print(cal_MRE(ResultM1[1:5]*3600,tdata[1:5]))\n",
    "\n",
    "#########\n",
    "#Model 2#\n",
    "#########\n",
    "print(cal_MAE(ResultM2[1:5]*3600,tdata[1:5]))\n",
    "print(cal_MRE(ResultM2[1:5]*3600,tdata[1:5]))\n",
    "print(cal_MAE(ResultM2*3600,tdata))\n",
    "print(cal_MRE(ResultM2*3600,tdata))\n",
    "#########\n",
    "#Model 3#\n",
    "#########\n",
    "print(cal_MAE(ResultM3[1:5]*3600,tdata[1:5]))\n",
    "print(cal_MRE(ResultM3[1:5]*3600,tdata[1:5]))\n",
    "print(cal_MAE(ResultM3*3600,tdata))\n",
    "print(cal_MRE(ResultM3*3600,tdata))\n",
    "#########\n",
    "#Model 4#\n",
    "#########\n",
    "print(cal_MAE(ResultM4[1:5]*3600,tdata[1:5]))\n",
    "print(cal_MRE(ResultM4[1:5]*3600,tdata[1:5]))\n",
    "print(cal_MAE(ResultM4*3600,tdata))\n",
    "print(cal_MRE(ResultM4*3600,tdata))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
